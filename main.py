# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1444OC9iGXZdsLHAHxViKeeowIekOLwac
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA
import plotly.express as px
import tensorflow as tf
from tensorflow import keras
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline

"""### Data Import"""

train_data = pd.read_csv('kaggle_train.csv')  # reading train data


x_train = train_data.drop('Labels', axis=1)   # extracting training data by dropping lable coloumn



y_train = train_data['Labels']  # extracting labels



"""# Data Preprocessing"""

train_data.info()

train_data.describe()   # data overview

train_data['Labels'].value_counts()     # checking class imbalance

"""Very minute class imbalance, so no need for oversampling"""

train_data.isnull().sum()   # finding null values if exist

"""There is no null value, so no need for data imputation

### Outlier Analysis
"""

z_scores = (x_train - x_train.mean())/x_train.std()   #calculating z-scores for each coloumn



z_scores.describe()   # z-scores describing

fig = px.box(x_train, y=x_train.columns, title="Boxplot of Z-Scores of Features")  # generating box-plot
fig.show()

# for each column make a list of rows where the z-score is > 3 or < -3
outlier_rows = []
for col in z_scores.columns:
    outlier_rows.append(z_scores[(z_scores[col] > 1.5) | (z_scores[col] < -1.5)].index.tolist())

print(outlier_rows)

# make a list of elements which are present in more than 50% sublists in outliner_rows list

sus_rows = []

for i in range (1, 252):
    sum_i = 0
    for list in outlier_rows:
        if i in list:
            sum_i += 1
    if sum_i >= len(outlier_rows) * 0.15:
        sus_rows.append(i)

print(sus_rows)

# remove sus_rows from x_train and y_train

x_train = x_train.drop(sus_rows, axis=0)
y_train = y_train.drop(sus_rows, axis=0)

"""We removed those data points who apeared in more than 15% (=48 features / 318 features) as a outliner"""



y_train = y_train.reset_index(drop=True)   # resetting index to correct their order



"""### Data Normalization"""

normalizer = MinMaxScaler()    #using min-max scaler for normalization

x_train = pd.DataFrame(normalizer.fit_transform(x_train))   #transforming the data



"""### Feature Analysis"""

pca = PCA(n_components=50)   # using PCA for feature selection and reducing them to 50

x_train_pca = pca.fit_transform(x_train)   # transforming the data according to PCA



x_train_pca = pd.DataFrame(x_train_pca)



x_train_pca.describe()

x_train_pca.isnull().sum()  # checking null values if exist

# number of 0 values in each column

x_train_pca.isin([0]).sum()

"""# Model Training"""

# number of rows in x_train

len(x_train)

len(y_train)

model = keras.Sequential(
    [
        keras.Input(shape=(50,)),
        keras.layers.Dense(100, activation="relu"),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(100, activation="relu", kernel_regularizer=keras.regularizers.l2(0.01)),
        keras.layers.Dense(1, activation="sigmoid", kernel_regularizer=keras.regularizers.l2(0.01))
    ]
)   # applying ANN model

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train_pca, y_train, epochs=1000, batch_size=5)    #fitting the model

# plot loss and accuracy

plt.plot(model.history.history['loss'])

"""**Random Forest Model**"""

rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)   # using randomforest classifier

rf.fit(x_train_pca, y_train)

rf.score(x_train_pca, y_train)

"""**SVC Classifier**"""

clf = make_pipeline(StandardScaler(), SVC(gamma="auto"))   # using SVC classifier
clf.fit(x_train_pca,y_train)
clf.score(x_train_pca, y_train)

"""# Testing Model

### Importing Test Data
"""

test = pd.read_csv('kaggle_test.csv')

# do same PCA and MinMaxScaler on test data

id_test = test['ID']

test = test.drop('ID', axis=1)

test = pd.DataFrame(normalizer.transform(test))



test_pca = pca.transform(test)



test_pca = pd.DataFrame(test_pca)



model.evaluate(test_pca)

predictions = model.predict(test_pca)

predictions = pd.DataFrame(predictions)

predictions = predictions.round().astype(int)

# predictions = clf.predict(test_pca)

# predictions = pd.DataFrame(predictions)

# predictions = predictions.round().astype(int)

output = pd.concat([id_test, predictions.round().astype(int)], axis=1)

output.columns = ['ID', 'Labels']



output.to_csv('ANN_output.csv', index=False)  # saving output to csv file